{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"RAW_interactions.csv\")\n",
    "data = pd.read_csv('review.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['stars'] != 0]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'sentiment' based on the 'stars' column\n",
    "# Negative (0) for stars <= 3, Positive (1) for stars > 3\n",
    "data['sentiment'] = data['stars'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "\n",
    "# Check the distribution of the new sentiment classes\n",
    "print(data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the distribution of the new sentiment classes\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Create a bar graph for sentiment distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'green'])\n",
    "plt.title(\"Sentiment Class Distribution\")\n",
    "plt.xlabel(\"Sentiment Class (0 = Negative, 1 = Positive)\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def normalize_repeated_characters(word):\n",
    "#     # Replace repeated characters with a single instance\n",
    "#     return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "\n",
    "# def remove_repeated_words(words):\n",
    "#     # Remove consecutive repeated words\n",
    "#     normalized_words = []\n",
    "#     for i, word in enumerate(words):\n",
    "#         if i == 0 or word != words[i - 1]:\n",
    "#             normalized_words.append(word)\n",
    "#     return normalized_words\n",
    "\n",
    "# def normalize_spelling(word, base_words):\n",
    "#     # Normalize spelling using Levenshtein distance\n",
    "#     for base_word in base_words:\n",
    "#         if edit_distance(word, base_word) <= 2:\n",
    "#             return base_word\n",
    "#     return word  # Return original word if no close match is found\n",
    "\n",
    "# def PreprocessingText(text, base_words):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#     if isinstance(text, str):  # Ensure the input is a string\n",
    "#         # Remove HTML tags, special characters, and digits\n",
    "#         text = re.sub(r'&\\w+;', ' ', text)  # Remove HTML entities\n",
    "#         text = re.sub(r'<[^>]+>', '', text)  # Matches anything between < and >\n",
    "#         text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "#         text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "#         text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "\n",
    "#         # Lowercase the text\n",
    "#         text = text.lower()\n",
    "\n",
    "#         # Tokenize and remove stopwords\n",
    "#         words = nltk.word_tokenize(text)\n",
    "#         words = [word for word in words if word not in stop_words]\n",
    "\n",
    "#         # Normalize repeated characters\n",
    "#         words = [normalize_repeated_characters(word) for word in words]\n",
    "\n",
    "#         # Remove repeated words\n",
    "#         words = remove_repeated_words(words)\n",
    "\n",
    "#         # Normalize spelling\n",
    "#         words = [normalize_spelling(word, base_words) for word in words]\n",
    "\n",
    "#         return \" \".join(words)\n",
    "\n",
    "#     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessingText(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        # Remove HTML tags, special characters, and digits\n",
    "        text = re.sub(r'&\\w+;', ' ', text)  # Remove HTML entities\n",
    "        text = re.sub(r'<[^>]+>', '', text) # Matches anything between < and >\n",
    "        text = re.sub(r'\\s+', ' ', text)    # Remove extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)     # Remove numbers\n",
    "\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize and remove stopwords\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        # removing HTML tags\n",
    "\n",
    "        return \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function\n",
    "data['cleaned_text'] = data['text'].apply(PreprocessingText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Few Enteries from Postive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `df` is your DataFrame and `sentiment` is the target column\n",
    "positive_rows = data[data['sentiment'] == 1]  # Filter rows where sentiment is positive\n",
    "negative_rows = data[data['sentiment'] == 0]  # Filter rows where sentiment is negative\n",
    "\n",
    "positive_rows_to_keep = positive_rows.sample(n=8000, random_state=42)  # Select 7,000 rows to drop\n",
    "positive_rows = positive_rows.drop(positive_rows_to_keep.index)  # Remove those 7,000 rows\n",
    "\n",
    "balanced_df = pd.concat([positive_rows, negative_rows])  # Combine reduced positive rows with negative rows\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index\n",
    "\n",
    "balanced_df = balanced_df.dropna(subset=['cleaned_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The suitable Method for Oversampling and Undersamplying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.under_sampling import AllKNN\n",
    "import pandas as pd, numpy, string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "#Remove Special Charactors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the f1 Score\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid , valid_y, plotLabel = \"Confusion Matrix\"):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the Training labels on validation dataset\n",
    "    y_train_pred = classifier.predict(feature_vector_train)  \n",
    "    f1_train = metrics.f1_score(label, y_train_pred)\n",
    "\n",
    "    # Evaluate on testing set\n",
    "    y_test_pred = classifier.predict(feature_vector_valid)\n",
    "    f1_test = metrics.f1_score(valid_y, y_test_pred)\n",
    "\n",
    "    print(f\"Training F1-Score: {f1_train:.4f}\")\n",
    "    print(f\"Testing F1-Score: {f1_test:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(valid_y, y_test_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(plotLabel)\n",
    "    plt.show()\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello how aring y'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello how aring you\"\n",
    "\n",
    "nltk.word_tokenize(text)\n",
    "\n",
    "if isinstance(text, str):\n",
    "    print(\"hello\")\n",
    "\n",
    "stemmer_obj = PorterStemmer()\n",
    "\n",
    "stemmer_obj.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tfidfVectorizationProcess( Xdata , DataLabels ):\n",
    "    \"\"\"\n",
    "    Splits the data and applies TF-IDF vectorization.\n",
    "\n",
    "    Args:\n",
    "        Xdata (iterable): Text data for vectorization.\n",
    "        DataLabels (iterable): Labels corresponding to the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: TF-IDF transformed training and test sets (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split( Xdata, \n",
    "                                                         DataLabels, \n",
    "                                                         test_size=0.3, \n",
    "                                                         random_state=42)\n",
    "\n",
    "    tfidfVectorizer = TfidfVectorizer( ngram_range=(1, 3), \n",
    "                                        max_df=0.75, \n",
    "                                        min_df=10, \n",
    "                                        sublinear_tf=True)\n",
    "\n",
    "    X_train = tfidfVectorizer.fit_transform( X_train )\n",
    "    X_test = tfidfVectorizer.transform( X_test )\n",
    "    print(\"Shape After Vectorization! \")\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: TF-IDF Vectorization using the existing data\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),  max_df=0.75, min_df=10, sublinear_tf=True)  # Limit to top 5000 features for efficiency\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
    "y_label = data['sentiment']\n",
    "\n",
    "# Step 2: Train-test split using the existing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_label, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shape of the resulting splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import AllKNN\n",
    "from collections import Counter\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "\n",
    "\n",
    "# Apply All-KNN for undersampling\n",
    "all_knn = AllKNN(allow_minority=False)\n",
    "X_train_under, y_train_under = all_knn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the class distribution after undersampling\n",
    "print(\"Class distribution after All-KNN undersampling:\", Counter(y_train_under))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def apply_simple_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Apply SMOTE for oversampling the minority class.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Features of the training set (sparse or dense matrix).\n",
    "        y_train: Labels of the training set.\n",
    "    \n",
    "    Returns:\n",
    "        X_resampled: Resampled features.\n",
    "        y_resampled: Resampled labels.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(sampling_strategy={0: y_train.value_counts()[1]}, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "def apply_adasyn(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Apply ADASYN (Adaptive Synthetic Sampling) for oversampling the minority class.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Features of the training set (sparse or dense matrix).\n",
    "        y_train: Labels of the training set.\n",
    "    \n",
    "    Returns:\n",
    "        X_resampled: Resampled features.\n",
    "        y_resampled: Resampled labels.\n",
    "    \"\"\"\n",
    "    adasyn = ADASYN(random_state=777, sampling_strategy = 1.0)\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Print class distribution after ADASYN\n",
    "    print(\"Class distribution after ADASYN:\", Counter(y_resampled))\n",
    "    \n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_under , y_train_under = apply_smote_tomek(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under , y_train_under = apply_simple_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under , y_train_under = apply_adasyn(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(svm.LinearSVC(), X_train_under, y_train_under, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test == 1), np.sum(y_test == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')\n",
    "            , X_train_under, y_train_under, X_test, y_test, plotLabel=\"LR with SMOTE and TF-IDF(ngram_range=(1, 3),  max_df=0.75, min_df=10, sublinear_tf=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(GradientBoostingClassifier(random_state=42)\n",
    "            , X_train_under, y_train_under, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(svm.LinearSVC(), X_tfidf, y_label, cv=5, scoring='f1')\n",
    "print(f\"Cross-Validation F1-Scores: {cv_scores}\")\n",
    "print(f\"Mean F1-Score: {cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Oversample the data\n",
    "def oversample_data(balanced_df):\n",
    "    # Separate features and labels\n",
    "    X = balanced_df['text']\n",
    "    y = balanced_df['sentiment']\n",
    "    \n",
    "    # TF-IDF requires numerical input, so we oversample first\n",
    "    oversampler = RandomOverSampler(sampling_strategy={0: 7000}, random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X.to_frame(), y)  # Use `.to_frame()` for X\n",
    "    print(\"Class distribution after oversampling:\", Counter(y_resampled))\n",
    "    return X_resampled['text'], y_resampled\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = oversample_data(balanced_df)\n",
    "\n",
    "# Step 2: TF-IDF Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_resampled)\n",
    "\n",
    "# Step 3: Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "xsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Train Logistic Regression Model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predictions and F1 Score for Training Data\n",
    "y_pred_train = logistic_model.predict(X_train)\n",
    "f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "print(f\"Logistic Regression F1 Score (Train): {f1_train}\")\n",
    "\n",
    "# Step 6: Predictions and F1 Score for Test Data\n",
    "y_pred_test = logistic_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "print(f\"Logistic Regression F1 Score (Test): {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 2: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Display Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Precision, Recall, F1-Score Breakdown\n",
    "report = classification_report(y_test, y_pred_test, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Train Gradient Boosting Model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predictions and F1 Score for Training Data\n",
    "y_pred_train = gb_model.predict(X_train)\n",
    "f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "print(f\"Logistic Regression F1 Score (Train): {f1_train}\")\n",
    "\n",
    "# Step 6: Predictions and F1 Score for Test Data\n",
    "y_pred_test = gb_model.predict(X_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "print(f\"Logistic Regression F1 Score (Test): {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text, vectorizer, model):\n",
    "    # Preprocess the text\n",
    "    processed_text = PreprocessingText(text)\n",
    "    # Transform the text using the trained TF-IDF vectorizer\n",
    "    text_tfidf = vectorizer.transform([processed_text])\n",
    "    # Predict using the trained Logistic Regression model\n",
    "    prediction = model.predict(text_tfidf)\n",
    "    prediction_proba = model.predict_proba(text_tfidf)\n",
    "    return prediction[0], prediction_proba\n",
    "\n",
    "# Example text input\n",
    "new_text = \"Very very very good that it smells like old\"\n",
    "\n",
    "# Predict sentiment for the new text\n",
    "predicted_label, predicted_proba = predict_sentiment(new_text, tfidf_vectorizer, logistic_model)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted Sentiment: {'Positive' if predicted_label == 1 else 'Negative'}\")\n",
    "print(f\"Prediction Probabilities: {predicted_proba}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after reset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "def build_lstm_model(input_dim, output_dim, input_length):\n",
    "    \"\"\"\n",
    "    Build a simple LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    input_dim (int): Size of the vocabulary.\n",
    "    output_dim (int): Dimension of the embedding layer.\n",
    "    input_length (int): Length of the input sequences.\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length),  # Embedding layer\n",
    "        LSTM(64, return_sequences=False),  # LSTM layer with 64 units\n",
    "        Dropout(0.2),  # Dropout for regularization\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 5000  # Vocabulary size for embedding layer\n",
    "embedding_dim = 64  # Embedding vector size\n",
    "max_sequence_length = 100  # Assume fixed max sequence length for input\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "lstm_model = build_lstm_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,  # Training data\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    validation_data=(X_test, y_test)  # Validation set\n",
    ")\n",
    "\n",
    "# Plot Training and Validation AUC\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC and Loss Curves\n",
    "def plot_training_curves(history):\n",
    "    # Extract data\n",
    "    auc_train = history.history['auc']\n",
    "    auc_val = history.history['val_auc']\n",
    "    loss_train = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "    epochs = range(1, len(auc_train) + 1)\n",
    "    \n",
    "    # Create subplots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # AUC Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, auc_train, label='Train AUC')\n",
    "    plt.plot(epochs, auc_val, label='Validation AUC')\n",
    "    plt.title('AUC Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss_train, label='Train Loss')\n",
    "    plt.plot(epochs, loss_val, label='Validation Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot curves\n",
    "plot_training_curves(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final CLear Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
